{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n0gaa/oscillating_mindfully/blob/main/Oscillating_Mindfully.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcpTCGWGc9KH"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C53E_Fh8Tr6z"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn --upgrade\n",
        "!pip install numpy\n",
        "!pip install tensorflow==2.3\n",
        "!pip install mne\n",
        "!pip install shap\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDi9DER46FVX"
      },
      "outputs": [],
      "source": [
        "# define CONSTS\n",
        "K = 2\n",
        "top_features_count = 10\n",
        "\n",
        "labels_file_path = \"/content/gdrive/My Drive/FA Research/processed/labels_ht_vip_ext.txt\"\n",
        "x_test_file_path = '/content/gdrive/My Drive/FA Research/models/XGBoost/x_test.csv'\n",
        "y_test_file_path = '/content/gdrive/My Drive/FA Research/models/XGBoost/y_test.npy'\n",
        "x_train_file_path = '/content/gdrive/My Drive/FA Research/models/XGBoost/x_train.csv'\n",
        "y_train_file_path = '/content/gdrive/My Drive/FA Research/models/XGBoost/y_train.npy'\n",
        "\n",
        "all_features = ['delta_fp1','delta_fpz','delta_fp2','delta_af3','delta_afz','delta_af4','delta_f1','delta_fz','delta_f2',\n",
        "                                             'theta_fp1','theta_fpz','theta_fp2','theta_af3','theta_afz','theta_af4','theta_f1','theta_fz','theta_f2',\n",
        "                                             'alpha_fp1','alpha_fpz','alpha_fp2','alpha_af3','alpha_afz','alpha_af4','alpha_f1','alpha_fz','alpha_f2',\n",
        "                                             'beta_fp1','beta_fpz','beta_fp2','beta_af3','beta_afz','beta_af4','beta_f1','beta_fz','beta_f2',\n",
        "                                             'gamma_fp1','gamma_fpz','gamma_fp2','gamma_af3','gamma_afz','gamma_af4','gamma_f1','gamma_fz','gamma_f2',\n",
        "                                             'c01_delta','c01_theta','c01_alpha','c01_beta','c01_gamma',\n",
        "                                             'c02_delta','c02_theta','c02_alpha','c02_beta','c02_gamma',\n",
        "                                             'c03_delta','c03_theta','c03_alpha','c03_beta','c03_gamma',\n",
        "                                             'c04_delta','c04_theta','c04_alpha','c04_beta','c04_gamma',\n",
        "                                             'c05_delta','c05_theta','c05_alpha','c05_beta','c05_gamma',\n",
        "                                             'c06_delta','c06_theta','c06_alpha','c06_beta','c06_gamma',\n",
        "                                             'c07_delta','c07_theta','c07_alpha','c07_beta','c07_gamma',\n",
        "                                             'c08_delta','c08_theta','c08_alpha','c08_beta','c08_gamma',\n",
        "                                             'c12_delta','c12_theta','c12_alpha','c12_beta','c12_gamma',\n",
        "                                             'c13_delta','c13_theta','c13_alpha','c13_beta','c13_gamma',\n",
        "                                             'c14_delta','c14_theta','c14_alpha','c14_beta','c14_gamma',\n",
        "                                             'c15_delta','c15_theta','c15_alpha','c15_beta','c15_gamma',\n",
        "                                             'c16_delta','c16_theta','c16_alpha','c16_beta','c16_gamma',\n",
        "                                             'c17_delta','c17_theta','c17_alpha','c17_beta','c17_gamma',\n",
        "                                             'c18_delta','c18_theta','c18_alpha','c18_beta','c18_gamma',\n",
        "                                             'c23_delta','c23_theta','c23_alpha','c23_beta','c23_gamma',\n",
        "                                             'c24_delta','c24_theta','c24_alpha','c24_beta','c24_gamma',\n",
        "                                             'c25_delta','c25_theta','c25_alpha','c25_beta','c25_gamma',\n",
        "                                             'c26_delta','c26_theta','c26_alpha','c26_beta','c26_gamma',\n",
        "                                             'c27_delta','c27_theta','c27_alpha','c27_beta','c27_gamma',\n",
        "                                             'c28_delta','c28_theta','c28_alpha','c28_beta','c28_gamma',\n",
        "                                             'c34_delta','c34_theta','c34_alpha','c34_beta','c34_gamma',\n",
        "                                             'c35_delta','c35_theta','c35_alpha','c35_beta','c35_gamma',\n",
        "                                             'c36_delta','c36_theta','c36_alpha','c36_beta','c36_gamma',\n",
        "                                             'c37_delta','c37_theta','c37_alpha','c37_beta','c37_gamma',\n",
        "                                             'c38_delta','c38_theta','c38_alpha','c38_beta','c38_gamma',\n",
        "                                             'c45_delta','c45_theta','c45_alpha','c45_beta','c45_gamma',\n",
        "                                             'c46_delta','c46_theta','c46_alpha','c46_beta','c46_gamma',\n",
        "                                             'c47_delta','c47_theta','c47_alpha','c47_beta','c47_gamma',\n",
        "                                             'c48_delta','c48_theta','c48_alpha','c48_beta','c48_gamma',\n",
        "                                             'c56_delta','c56_theta','c56_alpha','c56_beta','c56_gamma',\n",
        "                                             'c57_delta','c57_theta','c57_alpha','c57_beta','c57_gamma',\n",
        "                                             'c58_delta','c58_theta','c58_alpha','c58_beta','c58_gamma',\n",
        "                                             'c67_delta','c67_theta','c67_alpha','c67_beta','c67_gamma',\n",
        "                                             'c68_delta','c68_theta','c68_alpha','c68_beta','c68_gamma',\n",
        "                                             'c78_delta','c78_theta','c78_alpha','c78_beta','c78_gamma']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRSM-0PJUDfi"
      },
      "source": [
        "Get Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci1p0WkbUFoL"
      },
      "outputs": [],
      "source": [
        "def get_y_no_breath():\n",
        "  labels = np.loadtxt(labels_file_path, dtype=str)\n",
        "  labels_dict = {'HT_breath': 0, 'HT_medit': 0, 'HT_think': 1, \n",
        "                 'VIP_breath': 0, 'VIP_medit': 0, 'VIP_think': 1}\n",
        "\n",
        "  y = []\n",
        "  for label in labels:\n",
        "    y.append(labels_dict.get(label))\n",
        "\n",
        "  y_arr = np.array(y)\n",
        "  (l,c) = np.unique(y_arr, return_counts=True)\n",
        "  print(l,c)\n",
        "\n",
        "  return y_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzZoPKEhUIjf"
      },
      "outputs": [],
      "source": [
        "def get_y():\n",
        "  labels = np.loadtxt(labels_file_path, dtype=str)\n",
        "  \n",
        "  labels_dict = {'HT_breath': 2, 'HT_medit': 0, 'HT_think': 1, \n",
        "                 'VIP_breath': 2, 'VIP_medit': 0, 'VIP_think': 1}\n",
        "\n",
        "  y = []\n",
        "  for label in labels:\n",
        "    y.append(labels_dict.get(label))\n",
        "\n",
        "  y_arr = np.array(y)\n",
        "  (l,c) = np.unique(y_arr, return_counts=True)\n",
        "  print(l,c)\n",
        "\n",
        "  return y_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsyE1KXjUMEa"
      },
      "source": [
        "Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhI7zUPTUNIc"
      },
      "outputs": [],
      "source": [
        "def get_raw_epochs():\n",
        "  electrodes = ['Fp1','Fpz','Fp2','AF3','AFz','AF4','F1','Fz','F2']\n",
        "  path = \"/content/gdrive/My Drive/FA Research/processed/\"\n",
        "  file_suffix = \"_epochs_matrix_ht_vip_ext.npy\"\n",
        "  raw_epochs = []\n",
        "\n",
        "  for e in electrodes:\n",
        "    print(\"loading data from file: \" + path + e + file_suffix)\n",
        "    raw_epochs.append(np.load(path + e + file_suffix))\n",
        "\n",
        "  raw_epochs_all = np.dstack(raw_epochs)\n",
        "  print(raw_epochs_all.shape)\n",
        "  raw_epochs_all_transposed_expanded = np.expand_dims(raw_epochs_all, axis=3)\n",
        "  print(raw_epochs_all_transposed_expanded.shape)\n",
        "\n",
        "  return raw_epochs_all_transposed_expanded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krVZ3r3sUT-W"
      },
      "source": [
        "Get Subjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVhX3yXSUVBz"
      },
      "outputs": [],
      "source": [
        "def getSubjects():\n",
        "  import csv\n",
        "  sub = []\n",
        "\n",
        "  with open(\"/content/gdrive/My Drive/FA Research/processed/_metadata_ht_vip_ext.csv\", newline='') as metadata:\n",
        "    reader = csv.reader(metadata)\n",
        "    for row in reader:\n",
        "      if(row):\n",
        "        epochs = int(row[2])\n",
        "        for i in range(0,epochs):\n",
        "          sub.append(row[0])\n",
        "  \n",
        "  return sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhRQvUMZUYP7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def getCalibrationIndices(test_sub):\n",
        "  y_comp = get_y()\n",
        "  sub = getSubjects()\n",
        "  cal_idx = []\n",
        "\n",
        "  for t_sub in test_sub: # go over each test subject to add calibration epochs\n",
        "    idx = np.where(np.isin(sub,t_sub))[0] # get indices of current subject epochs within test set\n",
        "    labels = y_comp[idx] # get labels of current subject epochs\n",
        "    \n",
        "    for cond in set(labels):\n",
        "      if cond in[2]: #breath\n",
        "        cond_idx = np.where(np.isin(labels,cond))[0]\n",
        "        cal_start_idx = cond_idx[0]\n",
        "        cal_end_idx = cond_idx[0] + int((cond_idx.shape[0])*0.7) # add first 70% of breath epochs to training set for calibration\n",
        "        cal_idx.append(idx[cal_start_idx:cal_end_idx])\n",
        "      elif cond in [1]: #think\n",
        "        cond_idx = np.where(np.isin(labels,cond))[0]\n",
        "        cal_start_idx = cond_idx[0]\n",
        "        cal_end_idx = cond_idx[0] + int((cond_idx.shape[0])*0.3) # add first 30% of MW epochs to training set for calibration\n",
        "        cal_idx.append(idx[cal_start_idx:cal_end_idx])\n",
        "\n",
        "  cal_idx = [item for sublist in cal_idx for item in sublist] # flatten nested list\n",
        "  print(f\"Found {len(cal_idx)} epochs for calibration\")\n",
        "\n",
        "  random.shuffle(cal_idx)\n",
        "\n",
        "  return cal_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2hsM_JFVttu"
      },
      "source": [
        "################## MODEL COMPUTATIONS ####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcuwe4XTUbBJ"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from scipy import signal\n",
        "\n",
        "def butter_bandstop_filter(data, lowcut, highcut, fs, order):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = scipy.signal.butter(order, [low, high], btype='bandstop')\n",
        "    y = scipy.signal.filtfilt(b, a, data, axis=2)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QZUVbL-Uc1k"
      },
      "outputs": [],
      "source": [
        "def getFreqBand(epochs,min_freq,max_freq):\n",
        "  freq = np.zeros(shape=(epochs.shape[0],epochs.shape[2]))\n",
        "  i=0\n",
        "  while i < epochs.shape[0]:\n",
        "    j=0\n",
        "    while j < epochs.shape[2]:\n",
        "      f, Pxx = scipy.signal.periodogram(epochs[i,:,j,0], fs=256)\n",
        "\n",
        "      ind_min = np.argmax(f > min_freq) - 1\n",
        "      ind_max = np.argmax(f > max_freq) - 1\n",
        "      freqPower = np.trapz(Pxx.flatten()[ind_min: ind_max], f[ind_min: ind_max])\n",
        "      freq[i][j] = freqPower\n",
        "      j=j+1\n",
        "    i=i+1\n",
        "   \n",
        "  return freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXD859XVUf8t"
      },
      "outputs": [],
      "source": [
        "def getAlphaFreq(epochs):\n",
        "  return getFreqBand(epochs,8,12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_KzGS8VUhzq"
      },
      "outputs": [],
      "source": [
        "def getThetaFreq(epochs):\n",
        "  return getFreqBand(epochs,4,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OxywUKlUh2H"
      },
      "outputs": [],
      "source": [
        "def getDeltaFreq(epochs):\n",
        "  return getFreqBand(epochs,1,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sqdpF7VUldD"
      },
      "outputs": [],
      "source": [
        "def getBetaFreq(epochs):\n",
        "  return getFreqBand(epochs,12,30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-qiNzCMUh4m"
      },
      "outputs": [],
      "source": [
        "def getGammaFreq(epochs):\n",
        "  return getFreqBand(epochs,30,150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOmQO4_aUoa2"
      },
      "outputs": [],
      "source": [
        "from statistics import mean \n",
        "\n",
        "def getCoherence(epochs,electrodes):\n",
        "  coherence = np.zeros(shape=(epochs.shape[0],5))\n",
        "\n",
        "  i=0\n",
        "  while i < epochs.shape[0]:\n",
        "    f,c = signal.coherence(epochs[i,:,electrodes[0],0],epochs[i,:,electrodes[1],0], fs=256)\n",
        "    deltaC = mean(c[:4])\n",
        "    thetaC = mean(c[4:8])\n",
        "    alphaC = mean(c[8:12])\n",
        "    betaC = mean(c[12:30])\n",
        "    gammaC = mean(c[30:150])\n",
        "    coherence[i][0] = deltaC\n",
        "    coherence[i][1] = thetaC\n",
        "    coherence[i][2] = alphaC\n",
        "    coherence[i][3] = betaC\n",
        "    coherence[i][4] = gammaC\n",
        "  \n",
        "    i=i+1\n",
        "  return coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4ytgvg0UqBH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "def getFeaturesForEpochs(epochs):\n",
        "  delta = getDeltaFreq(epochs)\n",
        "  print(f\"shape of power: {delta.shape}\")\n",
        "  theta = getThetaFreq(epochs)\n",
        "  alpha = getAlphaFreq(epochs)\n",
        "  beta = getBetaFreq(epochs)\n",
        "  gamma = getGammaFreq(epochs)\n",
        "  coherences = []\n",
        "\n",
        "  for combo in combinations(range(9), 2):\n",
        "    coherences.append(getCoherence(epochs, combo))\n",
        "\n",
        "  x_features = np.hstack((delta,theta,alpha,beta,gamma, *coherences))\n",
        "  df = pd.DataFrame(data=x_features,columns=all_features)\n",
        "  print(f\"all features computed for epochs: {x_features.shape}, result is df of shape: {df.shape}\")\n",
        "  \n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEmZxLI7UuKp"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Normalizer,StandardScaler\n",
        "\n",
        "def standardize(x_train, x_test):\n",
        "  transformer = StandardScaler().fit(x_train)\n",
        "  x_train_sc = transformer.transform(x_train)\n",
        "  x_test_sc = transformer.transform(x_test)\n",
        "  x_train_sc = pd.DataFrame(x_train_sc, columns = x_train.columns)\n",
        "  x_test_sc = pd.DataFrame(x_test_sc, columns = x_test.columns)\n",
        "\n",
        "  return x_train_sc, x_test_sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOan_VDC5Tch"
      },
      "outputs": [],
      "source": [
        "def correlation_heatmap(train):\n",
        "    correlations = train.corr()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(100,100))\n",
        "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', cmap=\"YlGnBu\",\n",
        "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
        "    plt.show();\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2-LOXxm7mdn"
      },
      "outputs": [],
      "source": [
        "def printShapFeaturePattern(shap_values, feature):\n",
        "  shap.plots.scatter(shap_values[:,feature], color=None, ymin=-1, ymax=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtq3z9YFiGPN"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import shap\n",
        "\n",
        "def printTopFeatures(model, x_test_features_standardized):\n",
        "  # XGBoost\n",
        "  xgb.plot_importance(model,importance_type='gain', max_num_features=10)\n",
        "  plt.figure(figsize = (100, 100))\n",
        "  plt.show()\n",
        "\n",
        "  # SHAP\n",
        "  explainer = shap.Explainer(model)\n",
        "  shap_values = explainer(x_test_features_standardized)\n",
        "  shap.plots.beeswarm(shap_values,max_display=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWeD8OB3iPPi"
      },
      "outputs": [],
      "source": [
        "def getXgboostImportances(model):\n",
        "  xgb_importances = model.get_booster().get_score()\n",
        "  xgb_importances = dict(sorted(xgb_importances.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "  return xgb_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioVrhvMrjivI"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "def getShapImportances(model, x_test_features_standardized):\n",
        "  explainer = shap.Explainer(model)\n",
        "  shap_values = explainer(x_test_features_standardized)\n",
        "  feature_names = shap_values.feature_names\n",
        "  shap_df = pd.DataFrame(shap_values.values, columns=feature_names)\n",
        "  vals = np.abs(shap_df.values).mean(0)\n",
        "  shap_importances = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name', 'feature_importance_vals'])\n",
        "  shap_importances.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
        "  shap_importances = shap_importances.tail(shap_importances.shape[0] -1)\n",
        "\n",
        "  return shap_importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMoF4YadjuTL"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "def getPermutationImportances(model, x_test, y_test, metric):\n",
        "  perm_importances = permutation_importance(model, x_test, y_test, scoring=metric)\n",
        "  sorted_idx = perm_importances.importances_mean.argsort()\n",
        "  perm_importance_dict = dict(zip(x_test.columns[sorted_idx][::-1],perm_importances.importances_mean[sorted_idx][::-1]))\n",
        "\n",
        "  return perm_importance_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nRN840WlTeM"
      },
      "outputs": [],
      "source": [
        "def scaleFeatures(x_train, x_test):\n",
        "  print(x_train.shape)\n",
        "  print(x_test.shape)\n",
        "  x_test_scaled = np.zeros((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
        "  x_test_scaled[:] = x_test*1e6\n",
        "  x_train_scaled = np.zeros((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
        "  x_train_scaled[:] = x_train*1e6\n",
        "\n",
        "  return x_train_scaled, x_test_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGMZTxjqlrHj"
      },
      "outputs": [],
      "source": [
        "def shuffleTrainingSet(x_train, y_train, train_subjects):\n",
        "  train = list(zip(x_train, y_train, train_subjects))\n",
        "  random.shuffle(train)\n",
        "  x_train, y_train, train_subjects = zip(*train)\n",
        "  x_train = np.array(x_train)\n",
        "  y_train = np.array(y_train)\n",
        "\n",
        "  return x_train, y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGFjEfcunh7R"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def printConfusionMatrix(y_true, y_pred):\n",
        "  labels = ['medit', 'think']\n",
        "  cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "  cm = cm.astype(float)\n",
        "  cm_dict = {}\n",
        "  df1 = pd.DataFrame(cm, columns=labels, index=labels)\n",
        "  df2 = pd.DataFrame(cm/cm.sum(axis=1)[:,None], columns=labels, index=labels)\n",
        "  fig = plt.figure(figsize = (10,7))\n",
        "  sns.heatmap(df2, annot=True, fmt=\".2f\", cmap=\"summer\")\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dNnsPDyoWAT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "def printRocCurve(y_true, y_pred):\n",
        "  display = RocCurveDisplay.from_predictions(y_true, y_pred)\n",
        "  display.plot()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX6IEC1Hs3eH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def splitSetsBySubjects():\n",
        "  sub = getSubjects()\n",
        "  sub_set = list(set(sub))\n",
        "  print(f\"total subjects: {sub_set}\")\n",
        "  \n",
        "  random.shuffle(sub_set)\n",
        "  sub_split = np.split(sub_set, [int(len(sub_set)*0.2)])\n",
        "  test_sub = sub_split[0]\n",
        "  print(f\"Test subjects: {test_sub}\")\n",
        "  train_sub_min = sub_split[1]\n",
        "  train_index = [i for i, x in enumerate(sub) if x in train_sub_min]\n",
        "  train_sub = [x for i, x in enumerate(sub) if i in train_index]   \n",
        "  print(f\"training subjects: {train_sub}\")\n",
        "\n",
        "  return train_sub, test_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdU7KT6zuToS"
      },
      "outputs": [],
      "source": [
        "def getTrainingTestingSets():\n",
        "  sub = getSubjects()\n",
        "  train_sub, test_sub = splitSetsBySubjects()\n",
        "  \n",
        "  train_indices = [i for i, x in enumerate(sub) if x in train_sub]\n",
        "  test_indices = [i for i, x in enumerate(sub) if x in test_sub]\n",
        "  calibration_indices = getCalibrationIndices(test_sub)\n",
        "\n",
        "  x_comp = get_raw_epochs()\n",
        "  y_comp = get_y()\n",
        "\n",
        "  #add calibration epochs of test subjects to training set\n",
        "  y_total = get_y_no_breath()\n",
        "  x_train = np.array([x for i, x in enumerate(x_comp) if (i in train_indices) or i in calibration_indices])\n",
        "  x_test = np.array([x for i, x in enumerate(x_comp) if i in test_indices and i not in calibration_indices])\n",
        "  y_train = np.array([x for i, x in enumerate(y_total) if (i in train_indices) or i in calibration_indices])\n",
        "  y_test = np.array([x for i, x in enumerate(y_total) if i in test_indices and i not in calibration_indices])\n",
        "\n",
        "  return x_train, x_test, y_train, y_test, train_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h9GnT3GNt08"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, f1_score\n",
        "\n",
        "def computeScores(y_true, y_pred):\n",
        "  # calculate model performance scores\n",
        "  f1 = f1_score(y_true,y_pred)\n",
        "  acc = accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
        "  roc_auc = roc_auc_score(y_true, y_pred)\n",
        "  \n",
        "  # print visuals for model performance\n",
        "  printRocCurve(y_true, y_pred)\n",
        "  printConfusionMatrix(y_true, y_pred)\n",
        "\n",
        "  return acc, roc_auc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grEYqyxq_BH3"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def getModel():\n",
        "  return xgb.XGBClassifier(n_jobs=20,objective='binary:logistic', \n",
        "                                learning_rate=0.2, gamma=2, max_depth=10,\n",
        "                                colsample_bytree=1, min_child_weight=5, \n",
        "                                subsample=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iucaZlii_Kx4"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def trainModel(model, x_train, y_train):\n",
        "  weights = compute_sample_weight(class_weight='balanced',y=y_train)\n",
        "  model.fit(x_train, y_train, sample_weight=weights)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syRJTapaUvAA"
      },
      "outputs": [],
      "source": [
        "def runXgboost():  \n",
        "  x_train, x_test, y_train, y_test, train_sub = getTrainingTestingSets()\n",
        "\n",
        "  # prepare features for training\n",
        "  x_train_scaled, x_test_scaled = scaleFeatures(x_train, x_test)\n",
        "  x_train_scaled, y_train = shuffleTrainingSet(x_train_scaled, y_train, train_sub)\n",
        "  x_train_features = getFeaturesForEpochs(x_train_scaled)\n",
        "  x_test_features = getFeaturesForEpochs(x_test_scaled)\n",
        "  x_train_standardized, x_test_standardized = standardize(x_train_features, x_test_features)\n",
        "  x_test_standardized.to_csv(x_test_file_path, index=False, encoding='utf-8-sig')\n",
        "  x_train_standardized.to_csv(x_train_file_path, index=False, encoding='utf-8-sig')\n",
        "  np.save(y_test_file_path, y_test)\n",
        "  np.save(y_train_file_path, y_train)\n",
        "\n",
        "  print(\"Train data shape:\", x_train_standardized.shape, y_train.shape, \n",
        "        \"\\nTest data shape:\", x_test_standardized.shape, y_test.shape)\n",
        "\n",
        "  #XGBOOST create classifier\n",
        "  xgb_model = getModel()\n",
        "  xgb_model = trainModel(xgb_model, x_train_standardized, y_train)\n",
        "\n",
        "  y_pred = xgb_model.predict(x_test_standardized)\n",
        "\n",
        "  acc, roc_auc, f1 = computeScores(y_test, y_pred)\n",
        "  printTopFeatures(xgb_model, x_test_standardized)\n",
        "\n",
        "  # calculate importacnes for features\n",
        "  xgboost_importances = getXgboostImportances(xgb_model)\n",
        "  shap_importances = getShapImportances(xgb_model, x_test_standardized)\n",
        "  perm_importance_dict_roc = getPermutationImportances(xgb_model, x_test_standardized, y_test, \"roc_auc\")\n",
        "  perm_importance_dict_f1 = getPermutationImportances(xgb_model, x_test_standardized, y_test, \"f1\")\n",
        "\n",
        "  # call printShapFeaturePattern() here to print out SHAP graphs for specific features\n",
        "\n",
        "  return (acc,roc_auc,f1,xgboost_importances,shap_importances,perm_importance_dict_roc,perm_importance_dict_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCI_3XO09DOq"
      },
      "outputs": [],
      "source": [
        "def addImportantFeature(cur_list, weighted_scores, feature_index, top_features_count):\n",
        "  if cur_list in weighted_scores:\n",
        "    weighted_scores[cur_list] = weighted_scores[cur_list] + (top_features_count-feature_index)\n",
        "  else:\n",
        "    weighted_scores[cur_list] = (top_features_count-feature_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp9MpijwI7ew"
      },
      "outputs": [],
      "source": [
        "def printAggregatedScores(acc_scores, auc_scores, f1_scores):\n",
        "  print(\"Aggregated scores after K-fold\")\n",
        "  print(f\"AVG ACC SCORE: {statistics.mean(acc_scores)}, full scores: {acc_scores}\")\n",
        "  print(f\"AVG AUC-ROC SCORE: {statistics.mean(auc_scores)}, full scores: {auc_scores}\")\n",
        "  print(f\"AVG F1 SCORE: {statistics.mean(f1_scores)}, full scores: {f1_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P59P36A8JOs8"
      },
      "outputs": [],
      "source": [
        "def sumFeatureImportances(xgb_scores, shap_scores, roc_scores, f1_scores):\n",
        "  print(\"########### XGBOOST #############\")\n",
        "  xgb_scores = dict(sorted(xgb_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(xgb_scores)\n",
        "  print(\"########### SHAP #############\")\n",
        "  shap_scores = dict(sorted(shap_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(shap_scores)\n",
        "  print(\"########### PERMUTATION ROC #############\")\n",
        "  roc_scores = dict(sorted(roc_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(roc_scores)\n",
        "  print(\"########### PERMUTATION F1 #############\")\n",
        "  f1_scores = dict(sorted(f1_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "  print(f1_scores)\n",
        "\n",
        "  features = set([k for k in set(xgb_scores) | set(shap_scores) | set(roc_scores) | set(f1_scores)])\n",
        "  scores_total = {f: (xgb_scores.get(f, 0) + shap_scores.get(f, 0) + roc_scores.get(f, 0) + f1_scores.get(f, 0)) for f in features}\n",
        "  scores_total = {k: v for k, v in sorted(scores_total.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "  print(\"Summed Feature Importances:\")\n",
        "  print(scores_total)\n",
        "\n",
        "  return scores_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4z4kwFeI3H6"
      },
      "outputs": [],
      "source": [
        "def testDimensionalityReduction(summed_importances):\n",
        "  auc_scores_dimension_reduction = []\n",
        "  f1_scores_dimension_reduction = []\n",
        "  acc_scores_dimension_reduction = []\n",
        "  top_features = dict(list(summed_importances.items())[:top_features_count]).keys()\n",
        "  other_features = list(set(all_features) - set(top_features))\n",
        "\n",
        "  for i in range(K):\n",
        "    x_train_dimension_reduction = pd.read_csv(x_train_file_path).drop(labels=other_features,axis=1)\n",
        "    y_train_dimension_reduction = np.load(y_train_file_path)\n",
        "    x_test_dimension_reduction = pd.read_csv(x_test_file_path).drop(labels=other_features,axis=1)\n",
        "    y_test_dimension_reduction = np.load(y_test_file_path)\n",
        "    \n",
        "    model_dimension_reduction = getModel()\n",
        "    model_dimension_reduction = trainModel(model_dimension_reduction, x_train_dimension_reduction, y_train_dimension_reduction)\n",
        "    y_pred_dimension_reduction = model_dimension_reduction.predict(x_test_dimension_reduction)\n",
        "    acc_dimension_reduction, roc_auc_dimension_reduction, f1_dimension_reduction = computeScores(y_test_dimension_reduction, y_pred_dimension_reduction)\n",
        "\n",
        "    acc_scores_dimension_reduction.append(acc_dimension_reduction)\n",
        "    auc_scores_dimension_reduction.append(roc_auc_dimension_reduction)\n",
        "    f1_scores_dimension_reduction.append(f1_dimension_reduction)\n",
        "\n",
        "  print(\"******* Dimensinality Reduction Results for \", top_features_count, \"Top Features: \", top_features)\n",
        "  printAggregatedScores(acc_scores_dimension_reduction,auc_scores_dimension_reduction,f1_scores_dimension_reduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouFWDHjZvYgf"
      },
      "outputs": [],
      "source": [
        "import statistics\n",
        "\n",
        "xgb_weighted_scores = {}\n",
        "shap_weighted_scores = {}\n",
        "perm_weighted_scores_roc = {}\n",
        "perm_weighted_scores_f1 = {}\n",
        "auc_scores = []\n",
        "f1_scores = []\n",
        "acc_scores = []\n",
        "\n",
        "for i in range(K):\n",
        "  print(f\"~~~~~~ running model for iteration #{i} ~~~~~~\")\n",
        "  (acc,auc,f1,xgb_imp,shap_imp,perm_imp_roc,perm_imp_f1) = runXgboost()\n",
        "  \n",
        "  # accumulate scores\n",
        "  acc_scores.append(acc)\n",
        "  auc_scores.append(auc)\n",
        "  f1_scores.append(f1)\n",
        "\n",
        "  # build important features dictionary\n",
        "  for feature_index in range(top_features_count):\n",
        "    addImportantFeature(list(xgb_imp.keys())[feature_index], xgb_weighted_scores, feature_index, top_features_count)\n",
        "    addImportantFeature(shap_imp['col_name'].tolist()[feature_index], shap_weighted_scores, feature_index, top_features_count)\n",
        "    addImportantFeature(list(perm_imp_roc.keys())[feature_index], perm_weighted_scores_roc, feature_index, top_features_count)\n",
        "    addImportantFeature(list(perm_imp_f1.keys())[feature_index], perm_weighted_scores_f1, feature_index, top_features_count)\n",
        "\n",
        "printAggregatedScores(acc_scores, auc_scores, f1_scores)\n",
        "\n",
        "summed_importances = sumFeatureImportances(xgb_weighted_scores, shap_weighted_scores, perm_weighted_scores_roc, perm_weighted_scores_f1)\n",
        "testDimensionalityReduction(summed_importances)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Oscillating Mindfully.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+T9E7G3Trw16tFrbwkqU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}